# ADF Pipeline Automation - Complete Fix

## The Problem
The ADF pipeline `CopyProcessedDataToSQL` was not being created automatically during infrastructure deployment, causing Airflow DAGs to fail silently or require manual intervention.

## Root Cause
Terraform had `null_resource` provisioners configured, but they were pointing to the wrong directory paths:
- **Wrong**: `cd '${path.module}/../notebook'; python create_adf_pipeline.py`
- **Correct**: `python '${path.module}/../scripts/adf/create_adf_pipeline.py'`

## The Solution

### 1. Fixed Terraform Provisioners (terraform/main.tf)

**SQL Table Creation:**
```hcl
resource "null_resource" "create_sql_table" {
  depends_on = [
    azurerm_mssql_database.main,
    azurerm_mssql_firewall_rule.allow_current_ip
  ]

  triggers = {
    database_id = azurerm_mssql_database.main.id
    script_hash = filemd5("${path.module}/../scripts/sql/create_sql_table.py")
  }

  provisioner "local-exec" {
    command     = "python '${path.module}/../scripts/sql/create_sql_table.py'"
    interpreter = ["PowerShell", "-Command"]
    on_failure  = continue
  }
}
```

**ADF Pipeline Creation:**
```hcl
resource "null_resource" "create_adf_pipeline" {
  depends_on = [
    azurerm_data_factory.main,
    azurerm_storage_account.adls,
    azurerm_mssql_database.main,
    azurerm_role_assignment.sp_data_factory_contributor,
    null_resource.create_sql_table
  ]

  triggers = {
    adf_id      = azurerm_data_factory.main.id
    script_hash = filemd5("${path.module}/../scripts/adf/create_adf_pipeline.py")
  }

  provisioner "local-exec" {
    command     = "python '${path.module}/../scripts/adf/create_adf_pipeline.py'"
    interpreter = ["PowerShell", "-Command"]
    on_failure  = continue
  }
}
```

**Key Improvements:**
- Fixed script paths to correct locations
- Added `script_hash` trigger to recreate if script changes
- Proper dependency chain ensures resources exist before running
- `on_failure = continue` allows manual fix if needed

### 2. Added Airflow Preflight Check (astro-airflow/dags/nyc_311_incremental_etl_azure.py)

New task `validate_adf_exists` runs before `trigger_adf_pipeline`:

```python
def validate_adf_exists(**context):
    """Preflight check: ensure the expected ADF pipeline exists before triggering"""
    from azure.mgmt.datafactory import DataFactoryManagementClient
    
    loader = AzureDataLoader()
    pipeline_name = "CopyProcessedDataToSQL"
    
    adf_client = DataFactoryManagementClient(
        credential=loader.credential,
        subscription_id=loader.subscription_id
    )
    
    try:
        pipeline = adf_client.pipelines.get(
            resource_group_name=loader.resource_group,
            factory_name=loader.adf_name,
            pipeline_name=pipeline_name
        )
        print(f"âœ“ ADF pipeline found: {pipeline_name}")
    except HttpResponseError:
        msg = (f"ADF pipeline '{pipeline_name}' not found. "
               "Ensure post-deploy step created it.")
        raise RuntimeError(msg)
```

**Task Flow:**
```
start â†’ extract â†’ transform â†’ load_azure â†’ create_table 
  â†’ validate_adf_exists â†’ trigger_adf_pipeline â†’ update_state â†’ cleanup â†’ end
```

### 3. Enhanced Error Handling (astro-airflow/include/Loading_Azure.py)

Changed exception handling to **fail the task** instead of returning error dict:

```python
except Exception as e:
    logger.error(f"Error triggering ADF pipeline: {e}")
    logger.error("ADF trigger failed. Common causes: "
                 "(1) Pipeline not found, (2) Factory not deployed, "
                 "(3) Auth/permissions issue.")
    # Raise exception to fail the Airflow task
    raise RuntimeError(f"ADF pipeline trigger failed: {str(e)}") from e
```

### 4. Updated Quickstart Script (terraform/scripts/quickstart.ps1)

Added automatic ADF pipeline creation after successful `terraform apply`:

```powershell
if ($confirm -eq "yes") {
    terraform apply
    
    # Post-apply: Create/Update ADF pipeline
    try {
        Write-Host "Creating/Updating ADF pipeline (post-apply step)..." -ForegroundColor Yellow
        $adfScript = Join-Path $repoRoot "scripts/adf/create_adf_pipeline.py"
        if (Test-Path $adfScript) {
            python $adfScript
            Write-Host "âœ“ ADF pipeline ensured" -ForegroundColor Green
        }
    } catch {
        Write-Host "âœ— Run scripts/adf/create_adf_pipeline.py manually." -ForegroundColor Red
    }
}
```

## How It Works Now

### Fresh Deployment

1. **Terraform Apply:**
   ```powershell
   cd terraform
   terraform apply -auto-approve
   ```

2. **What Happens Automatically:**
   - Azure resources created (ADLS, ADF, SQL Database)
   - Service Principal gets Data Factory Contributor role
   - `null_resource.create_sql_table` runs â†’ Creates `nyc_311_requests` table
   - `null_resource.create_adf_pipeline` runs â†’ Creates `CopyProcessedDataToSQL` pipeline
   - Quickstart script runs post-apply hook (backup)

3. **Airflow DAG:**
   - Extracts data from NYC 311 API
   - Transforms and loads to ADLS
   - `validate_adf_exists` checks pipeline exists â†’ **Passes**
   - `trigger_adf_pipeline` runs ADF â†’ Copies data to SQL
   - Updates state file for next incremental run

### Infrastructure Destroy and Recreate

```powershell
# Destroy everything
terraform destroy -auto-approve

# Recreate everything (all automation runs again)
terraform apply -auto-approve

# Start Airflow - everything just works
cd ../astro-airflow
astro dev start
```

## Testing the Fix

### Test Provisioners Independently

```powershell
cd terraform
./test_provisioners.ps1
```

This validates both scripts run successfully before doing full Terraform apply.

### Verify After Deployment

1. **Check ADF Pipeline Exists:**
   ```powershell
   az datafactory pipeline show \
     --resource-group urban-cities-rg \
     --factory-name urban-cities-adf \
     --name CopyProcessedDataToSQL
   ```

2. **Check SQL Table Exists:**
   ```powershell
   # Or use scripts/sql/check_sql_data.py
   python scripts/sql/check_sql_data.py
   ```

3. **Trigger Airflow DAG:**
   ```powershell
   cd astro-airflow
   astro dev run dags trigger nyc_311_incremental_etl_azure
   ```

4. **Watch DAG Progress:**
   - Open http://localhost:8080
   - DAG should complete all tasks successfully
   - `validate_adf_exists` will pass (green)
   - `trigger_adf_pipeline` will run and copy data to SQL

## Benefits

### âœ… Complete Automation
- No manual steps required after `terraform apply`
- ADF pipeline created automatically every time
- SQL table created automatically every time

### âœ… Fail-Fast Behavior
- Airflow preflight check catches missing pipeline immediately
- Clear error messages point to root cause
- Tasks fail visibly instead of silently passing

### âœ… Idempotent Operations
- Re-running scripts is safe (they update existing resources)
- Script hash triggers re-run when scripts change
- Destroy/recreate cycles work flawlessly

### âœ… Multiple Safety Nets
1. Terraform provisioners (primary)
2. Quickstart script post-apply hook (backup)
3. Airflow preflight validation (runtime check)
4. Manual script execution still works (fallback)

## Common Issues and Solutions

### Issue: Provisioner fails during terraform apply
**Cause:** Python not in PATH or dependencies missing
**Solution:** Run manually after apply:
```powershell
python scripts/sql/create_sql_table.py
python scripts/adf/create_adf_pipeline.py
```

### Issue: Airflow task validate_adf_exists fails
**Cause:** ADF pipeline wasn't created
**Solution:** 
```powershell
cd scripts/adf
python create_adf_pipeline.py
```

### Issue: Service Principal permissions error
**Cause:** Role assignment not yet propagated
**Solution:** Wait 60 seconds and retry, or manually assign:
```powershell
az role assignment create \
  --role "Data Factory Contributor" \
  --assignee <service-principal-object-id> \
  --scope /subscriptions/<sub-id>/resourceGroups/urban-cities-rg/providers/Microsoft.DataFactory/factories/urban-cities-adf
```

## File Changes Summary

| File | Change | Purpose |
|------|--------|---------|
| `terraform/main.tf` | Fixed provisioner paths, added script_hash triggers | Automatic ADF/SQL creation |
| `astro-airflow/dags/nyc_311_incremental_etl_azure.py` | Added validate_adf_exists task | Preflight check |
| `astro-airflow/include/Loading_Azure.py` | Changed to raise exceptions | Proper task failure |
| `terraform/scripts/quickstart.ps1` | Added post-apply hook | Backup automation |
| `README.md` | Updated deployment steps | Documentation |
| `terraform/test_provisioners.ps1` | New testing script | Validation tool |

## Deployment Checklist

- [x] Terraform provisioners point to correct paths
- [x] Script hash triggers ensure updates run
- [x] Airflow has preflight validation
- [x] Error handling fails tasks properly
- [x] Quickstart script has post-apply hook
- [x] README updated with new flow
- [x] Test script created for validation
- [x] All configurations validated

## Conclusion

**The ADF pipeline issue is now fully resolved with multiple layers of automation:**

1. **Primary**: Terraform `null_resource` provisioners create everything automatically
2. **Backup**: Quickstart script post-apply hook ensures it runs
3. **Validation**: Airflow preflight check catches any gaps at runtime
4. **Manual**: Scripts still work standalone if needed

**You can now:**
- Run `terraform apply` and everything sets up automatically
- Destroy and recreate infrastructure without manual steps
- Trust that Airflow will fail loudly if something is misconfigured
- Deploy to any environment with confidence

**No more manual ADF pipeline creation steps required!** ðŸŽ‰
